# Transformer Architecture: "Attention is All You Need"
## Overview
This project implements the Transformer model, as introduced in the "Attention is All You Need" paper by Vaswani et al., 2017. The Transformer model is widely used in natural language processing (NLP) tasks like machine translation, text summarization, and more.

## Key Features
### Attention Mechanism: 
The model uses self-attention to process sequences in parallel, improving efficiency and performance.
### Encoder-Decoder Architecture: 
The Transformer has an encoder that processes the input sequence and a decoder that generates the output sequence.
### Positional Encoding: 
Since the model doesn't use recurrence, positional encodings are added to input embeddings to retain order information.

## References
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. A., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS), 30.
